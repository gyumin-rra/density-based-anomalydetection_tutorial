# density-based-anomalydetection_tutorial
a simple tutorial for desity-based anomaly detection

이 repository는 anomaly detection을 처음 접해보신 분들을 위해 작성되었습니다. 우선 anomaly detection에 대한 개념적인 overview, 여러 알고리즘에 대한 설명, 그리고 이 중 density based anomaly detection의 알고리즘들을 파이썬의 여러 구현체를 이용해 직접 실습해보는 순서로 구성하였습니다. 그리고 이 repository의 이론적인 토대는 첨부드린 논문과 고려대학교 강필성 교수님의 [유튜브 강의](https://www.youtube.com/watch?v=ECgI1YVQpY8&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=14)를 참고하였음을 밝힙니다. 

## 목차
1. [Concepts of Anomaly Detection](#concepts-of-anomaly-detection)
2. [Algorithms of Anomaly Detection](#algorithms-of-anomaly-detection)
3. [Implementation](#implementation)
4. [Conclusion](#conclusion)

---

## Concepts of Anomaly Detection
Anomaly detection(이상치 탐지)는 데이터에서 abnormal(novel) data인 부분을 찾아내는 문제를 다루는 머신러닝의 한 분야입니다. 다른 보통의 머신러닝 분야 중 classification, regression, clustering과 같은 것들과는 조금 다른 알고리즘이라고 할 수 있죠. Anomaly detection에서의 anomaly를 부르는 말로는 anomlay, novelty, outlier 등이 있습니다. 여기서는 우선 anomaly(이상치)라는 말로 통일하도록 하겠습니다. Anomaly를 찾아내는 여러 알고리즘을 본격적으로 살펴보기에 앞서 anomaly detection에 관련한 여러 개념을 살펴보겠습니다. 일단 anomaly가 어떤 것인지 알아보죠.

### Anomaly(이상치)란?
여러 논문 등의 문헌에서 anomaly에 대해 정의내린 바를 살펴보면 아래와 같습니다. 
1. generated by a different mechanism(Hawkins, 1980)
2. instances that their true probability density is very low (Harmeling et al., 2006)
3. patterns in data that do not conform to a well-defined notion of normal behavior(Chandola et al., 2007)

결국, anomaly는 데이터셋에서 데이터의 전체적인 변수의 값(feature value)이나 패턴을 고려하였을 때 나머지 데이터에서 보이는 양상과는 다른 양상을 보이는 데이터라고 할 수 있습니다. 그 원인으로는 해당 데이터의 발생 원리가 여타 데이터와는 다르기 때문일 수도 있고, 해당 데이터의 발생 확률이 낮아서 그럴 수도 있겠죠. 아래 그림으로 살펴보면(Chandola et al., 2007) $O_1$, $O_2$, $O_3$에 해당하는 점들이 이상치라고 할 수 있습니다. 
<p align="center"><img src="https://user-images.githubusercontent.com/112034941/201617039-201a32d3-a54a-4ec4-92bb-4f8a32059f5e.png" height="350px" width="400px"></p>

그러면 우리는 왜 이러한 이상치를 찾아내야 할까요? 이상한 데이터를 찾아내는 게 무슨 의미가 있죠? 사실 이상치 탐지가 가지는 의미는 결국 이상치가 다른 것들과 다르고, 희귀하기 때문에 적절한 분석이 이뤄지면 귀중한 정보가 될수 있다는 잠재력에 있습니다. 예를 들어 금융사기를 찾아내는(fraud detection) 모델을 만들어 내려고 한다고 해보죠. 이러한 문제를 해결함에 있어 가장 특징적인 부분은 결국 이 금융사기(fraud)가 다른 일반적인 금융거래에 비해 희귀하게 일어나고, 다른 mechanism을 통해 일어나고, 결과적으로 다른 거래들과는 다른 양상을 나타낸다는 점이 될 것 입니다. 바로 앞서 살펴보았던 이상치의 정의와 딱 들어맞는 것이죠. 금융사기 이외에도 여러 현실세계의 문제들을 보면 이상치를 찾아내야 하는 경우가 많습니다. 희귀병을 예측한다거나, 반도체 불량을 탐지한다거나, 우주선의 결함을 찾아낸다거나 하는 문제들은 모두 이상치 탐지 모델로 풀기 적합한 문제로써의 특징을 가지고 있는 현실세계의 문제입니다. 

### Anomaly VS Noise
Anomaly와 noise의 차이에 대해서도 한번 알아보겠습니다. 사실 혼자서 나중에 개념정리를 하다보면 불현듯 헷갈리는 게 바로 noise와 anomaly의 차이입니다. 정의만 따져보자면 Noise란, 데이터 생성 과정에서의 어쩔 수 없이(측정장비의 한계, 인적 오류) 생겨나는 데이터의 random한 변동입니다. 그러니까 애초에 anomaly와의 개념과는 조금 차이가 있는 셈이죠. 예컨데 아래 그림과 같은 데이터에 선형 회귀식을 그린 경우를 보면 noise에 대해 이해하기 쉬울 것입니다.
<p align="center"><img src="https://user-images.githubusercontent.com/112034941/201709768-52fe4c6a-072c-4554-ad42-7fd13c4fbe81.png" height="500px" width="500px"></p>

위 같은 데이터에서 보시다시피 데이터는 선형의 회귀식, 즉 선형의 함수식이라는 데이터 생성 메커니즘을 따르지만 노이즈가 있기 때문에 완벽하게 회귀식을 따르지 않고 어느정도의 차이가 존재하게 됩니다. 그러면 anomaly는 noise와 무엇이 다를까요? 핵심적인 차이는 random하게 생기는 변동이 아니라 아예 다른 생성확률분포를 따른다고 생각될 정도로 다른 데이터와 그 양상이 다르다는 점입니다. 예를 들면 아래와 같죠.
<p align="center"><img src="https://user-images.githubusercontent.com/112034941/201713755-6ee7d519-b42f-4b54-a280-db02e07a57a2.png" height="500px" width="500px"></p>

그림을 보면 빨간 원의 데이터는 나머지 초록 원 안의 데이터와는 다른 양상을 보이며, 다른 생성 매커니즘(선형 회귀식)을 따르는 것으로 보이므로 빨간 원 안의 데이터는 이상치라고 할 수 있습니다. 이러한 이상치의 경우에는 사실 찾아내기가 매우 쉽죠. 그러나 현실에서는 다른 생성 매커니즘에 의해 생성된 건 맞지만(이상치인건 맞는데) 정상데이터와 이상치를 분별하기 쉽지 않은 경우도 있습니다. 예를 들어, 아래 그림의 경우 위의 빨간 원의 데이터를 초록 원쪽으로 평행이동 시켜만들어낸 결과입니다. 
<p align="center"><img src="https://user-images.githubusercontent.com/112034941/201715422-489104e5-5911-4a33-abbd-4d8f49025506.png" height="500px" width="500px"></p>

이러한 경우 때문에 noise와 anomaly가 헷갈리는 경우가 생깁니다. 위 그림에서 빨간 원 안의 데이터를 노이즈라고 해도 뭐 그냥 그렇구나 하고 넘어가는 경우도 있지 않겠습니까? 물론 어느정도의 차이는 존재하지만 말이죠. 때문에 anomaly를 noise와 구분하기 위해 다른 데이터들에서의 양상과는 차이가 존재하는 데이터라는 정의를 붙인 것으로 생각할 수 있겠습니다. 만약에 나머지 데이터와 양상에서의 차이가 존재하지 않는다면? 그럼 이러한 이상치 탐지 문제는 해결하기 어렵거나 불가능한 문제로, 그 양상의 차이를 구분할 수 있도록 하는 추가 변수나 분석방법을 찾아야 할 것입니다.

### Anomaly Detection VS Classification
이제 조금 알고리즘적인 이야기를 해보겠습니다. Anomaly에 대해 앞서 정의한 바에 따르면 하기 사항이 가정되었다고 볼 수 있습니다.
1. 이상치는 그를 제외한 나머지 데이터와는 다른 feature value와 패턴을 보인다.
2. 이상치는 정상데이터에 비하여 매우 적다.

이러한 이상치의 특성은 anomaly를 찾아내는 task를 다루는 알고리즘, 즉 anomaly detection을 단순한 classification과는 다르게 만듭니다. 만약 anomaly를 찾아내기 위해서 일반적인 classification 알고리즘을 적용한다고 해보면 앞서 제시된 이상치의 특성에 의해 일종의 highly imbalanced classification problem을 해결해야 하는데, 데이터 불균형이 매우 심할 때 이러한 문제를 해결하는 것은 쉽지 않습니다. 아래와 같은 예시를 보시면 왜 문제 해결이 쉽지 않은지 알 수 있습니다. 
![image](https://user-images.githubusercontent.com/112034941/201837557-8d8b8515-9029-4ee8-8bc2-28b5ae0ce3ee.png)

위 그림을 보면 이상치의 개수가 너무 적기 때문에 classification 과정에서의 일반화성능을 측정을 위해 test set을 분리하였을 때 train set에서 만들어진 결정경계면으로 test set의 성능을 높게 만드는 것이 매우 어려움을 알 수 있습니다. 쉽게 말해 이상치 탐지를 위해 classification을 활용하면 일반화 성능이 매우 낮을 가능성이 크다는 것이고, 이것이 바로 classification과 이상치 탐지의 방법론이 다른 첫 번째 이유입니다.

Classification 알고리즘을 이상치 탐지 문제에 적용하기 어려운 두번째 이유는, 이상치 탐지 문제가 발생하는 환경의 특성상 classification을 위한 label이 완벽하게 주어져 있지 않은 경우가 있을 수 있다는 것입니다. 예를 들어 반도체 wafer 불량 예측과 같은 문제를 해결해야 하는 경우, 어떤 wafer가 이상이 있는지에 대한 명확하게 정의된 ground truth label을 구하는 것은 불가능합니다. 나중에 뭐 wafer로 실제로 반도체를 만들어보고 불량인지 아닌지는 판단할 수 있겠지만 그것은 불량 '예측'을 하고자 하는 경우에는 수행하기 어려운 방법이겠죠. 이처럼 이상치 탐지가 필요한 상황에서는 label이 없는 상황이 존재하고, 이러한 경우에는 classification 알고리즘을 적용할 수 없습니다. 

### Main Considerations in Anomaly Detection
그렇다면 anomaly detection에서는 어떤 것을 고려해야 할까요? 크게 보면 3가지로, 아래와 같이 정리할 수 있습니다.
1. input data의 특징과 찾아내려는 anomaly의 유형
2. training 과정에서의 label의 사용 여부
3. 방법론의 접근법

#### input data의 특징과 찾아내려는 anomaly의 유형
Anomaly detection을 적용하려는 input dataset의 특징과 찾아내고자 하는 anomaly의 유형은 anomaly detection의 알고리즘에도 영향을 미치게 됩니다. 이상치 탐지 관점에서의 input data에서 가장 중요한 특징은 데이터 instance, 즉 객체 간의 연관성이 있냐는 것입니다. 이러한 차이를 가장 잘 보여주는 예시로 tabular data, sequential data, graph data를 살펴봅시다.
![image](https://user-images.githubusercontent.com/112034941/201942445-e02fc639-ec5f-4357-8f63-9ac479e17b83.png)

tabular 데이터는 우리가 흔히 보는 표 형태의 데이터이고, sequential data는 DNA 염기서열이나 음악소리, 시간에 따른 온도변화와 같이 데이터 객체가 시간 순으로 배열되어 시간에 따른 패턴이 존재하는 데이터입니다. 그리고 graph data는 데이터 객체 간의 위계가 존재하는 데이터로, 제품의 부품에 관련한 데이터나 가계도, 질병 분류 등에 관련한 데이터들이 이에 해당합니다. 때문에 위와 같이 tabular data의 경우 instance간 연관성이 존재하지 않으나 나머지 두 개의 경우에는 연관성이 존재한다고 할 수 있습니다.

데이터 객체간의 연관성은 우리가 데이터에서 찾아낼 수 있는 anomaly의 유형에 영향을 미치게 됩니다. 우선, anomaly의 유형을 분류하자면 크게 세가지로, point anomaly, contextual anomaly, collective anomaly로 나눌 수 있습니다.
![image](https://user-images.githubusercontent.com/112034941/201944083-9836bd2a-77c4-4c62-8dcf-15ffe32ebf66.png)

우선 point anomaly는 데이터 객체 하나가 다른 객체들이 보이는 feature value 등의 양상 면에서 이상치인 경우를 말합니다. 상기 그림 중 좌측 1번째 그림의 $O_1, O_2, O_3$가 바로 그것이죠. 그리고 contextual anomaly는 객체들간의 연관성이 존재할 때, 그러한 연관성에 의한 흐름적 측면에서 이상치인 경우를 말합니다. 상기 그림 중 좌측 2번째 그림이 바로 그 예시로, 월별 온도의 변화 그래프를 시간에 따라 나타내었을때, 붉은 원 안의 데이터 객체가 바로 contextual anomaly라고 볼 수 있습니다. 마지막으로 collective anomaly는 데이터 객체간의 연관성이 있는 상황에서 데이터 객체들의 "집합"이 이상치적인 패턴을 보이는 경우를 말합니다. 위 그림에서 마지막 그림과 같은 경우죠. 

결국 앞서 설명드린 바와 같이 input data에서 나타나는 데이터 객체 간의 연관성은 어떤 유형의 anomaly를 탐지할 수 있는지에 영향을 미치게 됩니다. Tabular data와 같이 객체 간 연관성이 없는 경우에는 주로 point anomaly를 탐지하는 경우가 많을 것이고, graph data나 sequential data 같은 경우에는 contextual anomaly, collective anomaly를 탐지하는 경우가 많을테니 그에 맞추어 이상치 탐지 방법론을 적용해야 합니다. 

*이 repository에서는 가장 기본적인 유형인 tabular data에서의 point anomaly를 중심으로 설명하도록 하겠습니다.*

#### Training 과정에서의 label 사용 여부
이상치 탐지 모델도 다른 task에서의 모델과 유사하게 training 과정을 거칩니다. 그리고 이상치 탐지의 경우에는 label을 모두 활용하거나(supervised), 반만 활용하거나(semi-supervised), 활용하지 않는(unsupervised) 알고리즘도 있습니다. 아래 그림을 보시죠.
![image](https://user-images.githubusercontent.com/112034941/201954115-385070ad-1b72-40a5-a10c-105af18e3d90.png)

이상치 탐지 모델에서의 학습을 위한 label은 해당 데이터가 이상치인지 아닌지, 즉 anomaly vs normal로 구분되는 binary label입니다. 그리고 training 과정에서 anomaly, normal label을 모두 활용하는 것을 supervised 모델이라고 합니다. Supervised 모델에서는 마치 일반적인 classification 모델과 유사하게 anomaly와 anomaly가 아닌 데이터를 구분하는 분류경계면을 만들어줍니다. 물론 이러한 경우의 모델은 앞서 classification 모델이 이상치 탐지를 해결하기 어려운 이유를 극복해야만 하는 경우가 많습니다(이러한 문제를 해결하기 위해서 주로 upsampling 방법을 쓰곤 합니다). Semi-supervised 방법론의 경우에는 데이터의 label 중 normal label만 사용합니다. 그래서 training 과정에서는 정상데이터에 대해서만 모델을 학습하고, test(inference) 과정에서 이상치가 포함된 데이터를 사용합니다. 때문에 이러한 알고리즘들은 조금 독특하게 학습이 진행됩니다. 대체로 training 과정에서 정상데이터는 정상데이터들끼리 군집을 형성할 것이라는 일종의 cluster assumption을 하는 경우가 많고, 그게 아니라면 모델 자체적인 기준에 의하여 학습을 하는 경우도 있습니다. 마지막으로 unsupervised 방법론의 경우에는 어떤 label도 필요하지 않습니다. 군집화 알고리즘, 재구축 에러, 거리, 밀도 등의 다양한 기준을 활용하여 이상치와 이상치가 아닌 것을 분류합니다. 상기 그림에서는 군집화 기반의 알고리즘을 사용했다고 할 수있겠습니다.

#### 방법론의 접근법
우선, 이 부분을 설명하기에 앞서 이러한 사항을 이상치 탐지의 알고리즘을 파악할 때 고려할 수 있다는 아이디어는 고려대학교 [강필성 교수님의 강의](https://www.youtube.com/watch?v=ECgI1YVQpY8&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=14)에서 영감을 얻었음을 밝힙니다. 
이상치 탐지 방법론의 접근법이라 함은 각 방법론이 기반으로 두고 있는 핵심이 무엇인가로 볼 수 있습니다. 크게 density, distance, model의 세가지로 나눌 수 있습니다. 각 방법론의 접근법에 따른 특징과 해당하는 방법론을 표로 정리해보면 아래와 같습니다.  

| 접근법          | 특징                                                         | 해당하는 알고리즘     | 
| :--            | :--                                                          | :--      |
| density        |  데이터의 분포를 고려하여 밀도가 적은 영역의 객체는 이상치로 판단 | Gaussian-density Estimation, Mixture of Gaussian, Kernel-density Estimation(parzen window), Local Outlier Factor(LOF) | 
| distance       |  거리 기반의 metric이 큰 경우의 객체는 이상치로 판단            | KNN based, Clustering based, PCA based | 
| model          |  사용된 모델에 기반한 metric에 따라 이상치 판단                | Auto-encoder based, SVM based, Isolation Forest | 

우선 density 기반의 방법론은 데이터의 밀도를 고려하여, 밀도가 별로 없는 데이터는 이상치로 판단하는 알고리즘입니다. 따라서 이 알고리즘은 "이상치는 주변에 데이터 객체가 별로 없을 것이다"라는 가정하에 있다고 할 수 있습니다. 그리고 distance 기반의 방법론들은 일종의 거리에 기반한 이상치 score 등을 이용합니다. 군집화를 하고 그로부터 거리가 멀면 이상치로 판단한다던지 하는 것이 그 예시로 볼 수 있을 것입니다. 마지막으로 model based 방법론은 모델 자체적인 anomaly 판단 기준, 예컨데 분류경계면, 모델 네트워크, 이상치 점수 등을 이용해서 이상치를 판단합니다. 각 알고리즘과 그 분류를 나열하면 아래와 같습니다.
1. **밀도 기반(density-based)**
    - Gaussian Density Estimation
    - Mixture of Gaussian(MoG)
    - Parzen Window Density Estimation
    - Local Outlier Factor(LOF)
2. **거리 기반(distance-based)**
    - K-nearest neighbor(KNN) based
    - Clustering based
    - PCA based
3. **모델 기반(model-based)**
    - Autoencoder based
    - 1-SVM
    - SVDD(support vector data description)
    - Isolation Forest

여기까지 anomaly detection에 대한 주요 개념들을 개략적으로 살펴보았습니다. 이제부터는 density based anomaly detection의 다양한 알고리즘들의 개념에 대해 살펴보겠습니다. 

---

## Algorithms of Anomaly Detection
앞서 설명드린 바와 같이, anomaly detection의 주요 고려사항 중에는 이 방법론의 핵심적 기반이 무엇인가에 대한 내용도 있습니다. 크게 밀도, 거리, 모델 기반으로 나눌 수 있는데, 이 repository에서는 밀도 기반의 알고리즘들을 설명하려 합니다. 설명드릴 알고리즘과 그 순서는 아래와 같습니다. 
1. Gaussian Density Estimation
2. Mixture of Gaussian(MoG)
3. Parzen Window Density Estimation
4. Local Outlier Factor(LOF)

### Gaussian Density Estimation
Gaussian density estimation을 한 문장으로 요약하면 아래와 같습니다.
> 데이터가 정규분포를 따른다 치면, 변두리에 있는 객체가 anomaly다!

![image](https://user-images.githubusercontent.com/112034941/202154326-a2faac9d-f2c5-4627-810e-0dabf3f83abb.png)
Gaussian density estimation은 데이터들이 모두 정규분포에 의해 생성되었다고 가정합니다. 때문에 우선 데이터를 이용하여 maximum likelihood estimation(최대우도추정)을 통해 데이터가 따르는 다변량 정규분포의 평균벡터와 공분산행렬을 도출합니다. 그리고 그 다변량 정규분포에 따라 데이터 객체가 발생할 확률을 계산하는데, 그 확률이 낮은 경우에는 이상치로 분류합니다. 그리고 위 그림과 같이, 정규 분포의 확률밀도함수 값이 낮은 영역은 정규분포의 평균 벡터에서 멀리 떨어진 영역입니다. 바로 **밀도가 낮은 영역**이죠. 때문에 이 방법론은 밀도 기반의 이상치 탐지 방법론이라고 할 수 있겠습니다.

방법론의 특징과 관련 이슈를 살펴보면 아래와 같습니다.
- gaussian densitiy estimation의 특징 및 관련 이슈
    1. 학습이 간단하다: 최대우도추정법에 의하여 주어진 데이터의 확률분포 값을 최대화하는 정규분포의 평균벡터와 공분산행렬의 closed form solution이 도출됩니다. 최대우도추정법의 해는 각각 표본 평균벡터와 표본 공분산 행렬과 동일하므로 $\vec{\mu} =(\mu_1, \mu_2, ..., \mu_d)$, $\Sigma={\left\lbrack \matrix{cov(X_1, X_1) & cov(X_1, X_2) & ... & cov(X_1, X_d) \cr cov(X_2, X_1) & cov(X_2, X_2) & ... & cov(X_2, X_d) \cr ... & ... & ... & ... \cr cov(X_d, X_1) & cov(X_d, X_2) & ... & cov(X_d, X_d)} \right\rbrack}$로 대입을 통해 구하면 됩니다. 
    2. 이상치에 대한 기준을 정하기 간편하다.
    3. 공분산 행렬의 조건(full VS diagonal VS spherical)에 따라 분포의 모양이 달라진다: 이 부분은 아래 그림을 통해 자세히 살펴보시죠.

![image](https://user-images.githubusercontent.com/112034941/202166820-2079d786-f3ec-4339-bb22-52b562a6d2b1.png)

위의 그림처럼, 만약 공분산 행렬이 대각원소만 존재하면서 크기가 동일한 spherical인 경우 분포가 원형이고, 대각원소만 존재하지만 크기가 다른 diagonal 공분산 행렬의 경우 분포가 축에 평행하게 늘어난 타원형이며, 대각원소와 그 이외의 원소도 존재하면서 크기가 다른 full 공분산 행렬의 경우에는 축에 평행하지 않은 방향으로 늘어난 정규분포를 나타내게 됩니다. 아무래도 full covariance matrix 조건을 가진 경우에 이상치 탐지가 더 정확하게 이뤄지겠죠.

### Mixture of Gaussian(MoG)
Mixture of gaussian을 한문장으로 요약하면 아래와 같습니다.
> 데이터가 여러 개의 정규분포로 생성됐다 치면, 그 모두의 변두리에 있는 객체가 anomaly다!

![image](https://user-images.githubusercontent.com/112034941/202185326-65284dda-d8e1-467a-854f-c6000bc62ed0.png)
MoG는 gaussian density estimation에서 한발짝 더 나아간 형태로, 데이터들이 **여러 개의** 정규분포의 **선형결합**에 의해 생성되었다고 가정합니다. 그리고 MoG에서는 최대 우도 추정이 아닌 EM 알고리즘(expectation-maximization)을 통해 데이터가 따르는 다변량 정규분포들의 최적의 선형결합을 추정하고, 앞서 제시된 gaussian density estimation과 유사하게 그 선형결합의 함수 값이 낮은 것을 이상치로 분류합니다. 상기 그림과 같이 이 함수 값이 낮은 영역은 데이터의 밀도가 낮은 영역이죠. 

방법론의 특징과 관련 이슈를 살펴보면 아래와 같습니다.
- MoG의 특징 및 관련 이슈
    1. Gaussian density estimation과 다르게 한번에 solution을 구할 수 없다: 이는 방법론의 특성 때문으로, EM 알고리즘의 특성상 여러 번의 반복적 계산을 통해 numerical opitmization을 수행해야 합니다. 
    2. 다른 이상치 탐지 방법론에 비해 추정이 간단하고 이상치에 대한 기준을 정하기 간편하다.
    3. Gaussian density estimation과 동일하게 공분산 행렬의 조건(full VS diagonal VS spherical)에 따라 분포의 모양이 달라진다: 이 부분도, 아래 그림을 통해 자세히 살펴보시죠.

![image](https://user-images.githubusercontent.com/112034941/202179555-836c78bd-0eee-4476-9a32-51bf2e55e392.png)

위의 그림에서 나타나듯 MoG에서의 공분산 행렬이 full/diagonal/spherical인 경우 만들어진 각각의 가우시안 분포의 모양이 full/diagonal/spherical 공분산 행렬을 가진 모양이 됩니다. 더불어, 상기 그림에서 보이지만 이상치 탐지의 정확도 측면에서는 full covariance matrix일수록 더 높은 정확도를 가지는 경향이 있습니다만 반대로 계산복잡도가 더 높아진다는 trade off가 존재합니다.

### Parzen Window Density Estimation
이 방법론은 두 문장으로 요약할 수 있습니다. 
> 만약에 가우시안 분포가정이 사라지면 어떡하지? 영역을 정의해서 확률밀도를 추정해보자!

![image](https://user-images.githubusercontent.com/112034941/202185100-c7255705-a09c-4e81-91c9-5c1c61eb532e.png)
우선 이 방법론은 사실 kernel density estimation의 일종입니다. 그리고 위에 요약한 바와 같이 kernel density estimation의 핵심 아이디어는 어떤 데이터 객체의 발생확률을 그 객체가 포함된 영역을 정의하여 그 밀도를 통해 추정할 수 있다는 것입니다. 수식적으로 이를 나타내자면, 어떤 객체 $x$의 발생확률은 그 객체가 속한 영역이 $R$, 그 영역의 hypervolume이 $V$, 전체 데이터 객체의 수가 $N$, 영역 $R$에 속한 다른 객체의 수가 $k$ 인 상황에서 $N \to \infty$, $area(R) \to 0$ 이면 $k/NV$에 수렴합니다. 따라서 $k$, 즉 데이터 객체가 속한 영역에 속한 다른 객체들의 수가 작을 수록, 다시 말해 **밀도가 작을수록** 데이터 객체의 발생확률이 작은 것이고, 따라서 밀도가 작은 영역의 객체는 이상치가 됩니다.

그리고 여기서 parzen window density estimation은 영역을 $h$ 길이의 변을 가진 hypercube로 정의하고, $k$를 $K(u)=1$ if $|u_j|<1/2 \forall j$, $o.w 0$로 정의한 뒤 이를 정규분포와 같은 원점 대칭의 unimodal 함수로 smoothing한다는 점이 추가됩니다. 그리고 이 방법론에서의 확률 밀도 함수는 $p(x)=\frac{1}{Nh^d}\sum_iK(\frac{x_i-x}{h})$입니다. 위 그림에서 나타나듯, $k$는 $x$를 무게중심으로 하는 $h$ 길이의 변을 가진 hypercube 안에 있는 객체 $x^i$의 경우에는 0 이상의 확률을 가지게 하고 무게중심에 가까울수록 높은 값을 가지게 되므로 결국 $x$에 가까운 객체가 많은 경우 $x$의 밀도 함수 값이 높아지고, $p(x)=\frac{1}{Nh^d}\sum_iK(\frac{x_i-x}{h})$에서 $h$가 분모에 있으므로 $h$가 커질 경우에는 모든 객체의 확률 값이 비슷해질 것입니다. 때문에 아래 그림과 같이 $h$에 따라 확률분포가 달라지게 됩니다.
![image](https://user-images.githubusercontent.com/112034941/202194428-01c37ca7-b861-43b2-890d-999d25b8cb26.png)

### Local Outlier Factor
이 방법론은 아래와 같이 요약할 수 있습니다. 
> 데이터 주변 밀도를 상대적으로, 거리를 통해 고려해서 이상치를 평가하자!

<p align="center"><img src="https://user-images.githubusercontent.com/112034941/202195284-620ef134-4c77-430e-aa5c-2e7e3ad82f3d.png" height="500px" width="500px"></p>

위 그림을 보면, 각 점 별로 숫자가 적혀 있음을 알 수 있습니다. 저 숫자가 바로 LOF로, LOF 알고리즘은 LOF가 높은 경우를 이상치로 분류합니다. 그런데, LOF가 높은 경우를 살펴보면 주변의 밀도가 낮다는 점, 그리고 밀도가 낮아질수록 훨씬 LOF가 높죠. 그림에서 1, 2, 3번을 비교해보면 1번의 경우 전체적으로 존재하는 두개의 군집과 모두 멀리 떨어져있는 반면 2번의 경우 아래쪽 군집에 비교적 가까운 것을 알 수 있습니다. 하지만 여전히 높은 가중치를 가지죠. 그리고 3번의 경우에는 바로 가까이에 있는 점 때문에 1번에 비해서는 약간 LOF가 낮게 나오는 것을 볼 수 있습니다. 

그러면 LOF는 어떤 식으로 계산될까요? 우선 계산 과정에서 이해해야 할 개념인 어떤 객체 $x$의 k-neighborhood, k-distance, k-reachability distance, 그리고 k-local reachability density입니다. 각각을 $N_k(x)$, $d_k(x)$, $rd_k(x)$, $lrd_k(x)$로 표기하겠습니다.
1. $N_k(x)$: 객체 $x$로부터의 거리가 작은 순으로 나열했을 때 동률을 허용한 k위까지의 데이터 객체의 집합입니다. 예를 들어 나로부터 거리가 1,10,100,100,100,1000 인 경우 나의 3-이웃($N_3$)는 1, 10, 100, 100, 100거리 만큼 떨어진 객체 5개가 됩니다. 
2. $d_k(x)$: $N_k$의 객체들의 거리 중 최대 거리입니다. 위의 예를 그대로 가져온다면, $d_3$는 100이 될 것입니다.
3. $rd_k(x,o)$: 만약 $o$가 $N_k$에 속한다면 $d_k(x)$, 아니라면 그냥 정의된 거리 metric에 따라 $x$부터 $o$까지의 거리가 됩니다. 
4. $lrd_k(x)$: $\frac{|N_k(x)|}{\sum_{o\in N_K(x)}rd_k(x,o)}$로 계산합니다. 때문에 밀도가 높아지면 결국 $d_k$가 작아지므로 $lrd_k(x)$는 커집니다.
5. $LOF_k(x)$: $\frac{\frac{1}{lrd_k(x)}\sum_{o\in N_k(x)}lrd_k(o)}{|N_k(x)|}$로 계산합니다(식이 작아 아래에 한번 더 적어두겠습니다). 앞서 정의된 바에 따르면 밀도가 작아지면 결국 $lrd_k(x)$는 작아지므로 $\frac{1}{lrd_k(x)}$는 커집니다. 만약 이 객체의 k-이웃이 군집을 이룬다면 $lrd_k(o)$도 커지게 되므로 결국 이러한 경우의 객체 $x$는 LOF가 크기 때문에 이상치가 되게 됩니다.
$$\frac{\frac{1}{lrd_k(x)}\sum_{o\in N_k(x)}lrd_k(o)}{|N_k(x)|}$$

결론적으로 보면, LOF가 큰 경우는 해당 객체 주변의 밀도에 비해 해당 객체의 k-이웃들의 밀도가 높은 경우이고 LOF가 낮은 겨우는 해당 객체 주변의 밀도에 비해 해당 객체의 k-이웃들의 밀도가 낮은 경우가 되겠습니다. 아래 그림 중 좌측이 LOF가 큰 대표적인 경우이고 우측이 낮은 경우가 되겠지요.
![image](https://user-images.githubusercontent.com/112034941/202227589-ff1810e8-6bdc-4d6a-bc88-1ccb009f7f9b.png)

여기까지 밀도기반의 이상치 탐지 알고리즘들을 살펴보았습니다. 이제 이 방법론들을 이용하여 실험을 진행해보겠습니다.

---

## Implementation
우선 실험에 앞서 필요한 모듈 등의 버젼은 아래와 같습니다.
| env_name   | version |
|------------|---------|
| python     | 3.8.3   |
| numpy      | 1.19.2  |
| matplotlib | 3.5.2   |
| pandas     | 1.4.3   |
| sklearn    | 1.1.1   |

사용할 데이터셋은 [MNIST](https://yann.lecun.com/exdb/mnist/)입니다. 현재(221117 기준) 어떤 이유인지 사이트에 로그인이 필요한 상황인데, 필요한 데이터를 미리 다운받은 것이 있어 이 repo에 올려두었으니 참고하시길 바랍니다. 이 MNIST 데이터 중 1 데이터 전부를 나머지 숫자들의 데이터 20개를 사용하여 1을 정상, 나머지는 이상치로 처리하였습니다. 그리고 이를 tabular 데이터로 전처리하여 앞서 살펴본 알고리즘들을 적용하였습니다. 전처리 코드는 아래와 같습니다.
```python
import pandas as pd
import numpy as np
import random
import gzip

# MNIST preprocessing to tabular data
with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:
    X_data = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28*28)

with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:
    digit_label = np.frombuffer(f.read(), np.uint8, offset=8)

X_data = pd.DataFrame(X_data)
X_data['digit_label'] = digit_label
```
이에 따라 정상:이상치의 비율은 약 6472:20(이상치 약 0.003%)입니다.

실험해볼 내용은 크게 4가지입니다.
1. Classification 적용: 현재 데이터에 RBF 커널을 사용한 SVM을 적용하여 데이터 분류를 수행해보겠습니다. 앞서 설명드린 내용에 따르면, 당연히 학습이 잘 되지 않아야할텐데 이를 확인해보는 과정입니다.
2. Gaussian density estimation VS Mixture of Gaussian: covariance, mixture의 수에 따른 시간 및 성능 변화  
3. Parzen window density estimation: h, kernel, cutoff에 따른 성능 변화 관찰
4. LOF 적용: k-neibor hood에 따른 성능변화 관찰

### RBF SVM Anomaly Detection
우선 classification 실험코드와 결과는 아래와 같습니다.
1. 전처리 코드
```python
total_data = pd.concat([normal_data.iloc[:,:784], abnormal_data.iloc[:,:784]])
total_data['abnormal'] = np.repeat(0,normal_data.shape[0]).tolist()+np.repeat(1,abnormal_data.shape[0]).tolist()
total_data = total_data.reset_index(drop=True)
```
2. k-fold 함수와 평가함수. 실험은 5-fold로 진행하였습니다.  
```python
def kfold(data, fold, seed):
    import random
    idx_set_1 = data.loc[data.abnormal== 1].index.tolist()
    idx_set_0 = data.loc[data.abnormal == 0].index.tolist()
    size_1 = round(len(idx_set_1)/fold)
    size_0 = round(len(idx_set_0)/fold)
    folded_idx_set = []
    for i in range(fold):
        if (i == fold-1):
            folded_idx_set.append(idx_set_1+idx_set_0)
        else:
            random.seed(seed)   
            folded_idx_set.append(list(random.sample(idx_set_1, size_1))+list(random.sample(idx_set_0, size_0)))
            idx_set_1 = list(set(idx_set_1)-set(folded_idx_set[len(folded_idx_set)-1]))
            idx_set_0 = list(set(idx_set_0)-set(folded_idx_set[len(folded_idx_set)-1]))
    
    return folded_idx_set
    
from sklearn.metrics import confusion_matrix
def evaluation(fold_num, model, trn_X, trn_y, tst_X, tst_y, data):
    y_pred = model.predict(trn_X)
    tn, fp, fn, tp = confusion_matrix(trn_y, y_pred).ravel()
    data.loc[fold_num, 'trn_accuracy'] = (tp+tn)/(tn+fp+fn+tp)
    data.loc[fold_num, 'trn_f1'] = 2*tp/(fp+2*tp+fn)
    

    y_pred = model.predict(tst_X)
    tn, fp, fn, tp = confusion_matrix(tst_y, y_pred).ravel()
    data.loc[fold_num, 'tst_accuracy'] = (tp+tn)/(tn+fp+fn+tp)
    data.loc[fold_num, 'tst_f1'] = 2*tp/(fp+2*tp+fn)

    return data

eval_data = pd.DataFrame([[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]], columns=['trn_accuracy', 'trn_f1', 'tst_accuracy', 'tst_f1'])

kfold_idx_set = kfold(data=total_data, fold=5, seed=1117)

trn_X = []
trn_y = []
tst_X = []
tst_y = []
index_set = total_data.index.tolist()

for i in range(5):
    total_idx = set(index_set)
    trn_X.append(total_data.loc[list(total_idx-set(kfold_idx_set[i])), list(range(0,784))])
    trn_y.append(total_data.loc[list(total_idx-set(kfold_idx_set[i])), 'abnormal'])
    tst_X.append(total_data.loc[kfold_idx_set[i], list(range(0,784))])
    tst_y.append(total_data.loc[kfold_idx_set[i]]['abnormal'])

```
3. SVM 실험 코드. cost 계수는 1로 정하였습니다.
```python
from sklearn.svm import SVC
for i in range(5):
    fold_num = i
    model = SVC(kernel='rbf', C=1)
    model.fit(trn_X[fold_num], trn_y[fold_num])
    eval_data = evaluation(fold_num, model, trn_X[fold_num], trn_y[fold_num], tst_X[fold_num], tst_y[fold_num], eval_data)

eval_data
```
실험결과는 아래와 같습니다.
<p align="center"><img src="https://user-images.githubusercontent.com/112034941/202385646-1953b310-194c-404d-b33c-8d3d5b94c400.png" height="300px" width="700px"></p>

우선 데이터 불균형이 심각하고, 이상치 탐지가 중요한 상황이므로 주로 볼 평가지표는 f1-measure입니다. 그리고 앞서 이론 파트에서 언급드린 바와 같이 train set에서의 성능은 어느정도 확보되지만 test set의 성능은 0에 수렴하는 것을 볼 수 있습니다.

데이터에 대한 전체적 embedding을 직관적으로 확인해보기 위해 t-SNE로 2차원 정사영을 진행해보았습니다. 코드와 결과는 아래와 같습니다.
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import warnings 
warnings.filterwarnings("ignore") # warning 무시

tsne = TSNE(n_components=2, random_state = 1117)
tsne_data = tsne.fit_transform(total_data[list(range(0,784))])
tsne_data = pd.DataFrame(tsne_data, columns=['z1', 'z2'])

plt.figure(figsize=(10,10))
plt.title('TSNE')
plt.scatter(tsne_data.z1, tsne_data.z2, c=total_data.abnormal, cmap=plt.cm.brg, alpha=0.7)
```
![image](https://user-images.githubusercontent.com/112034941/202386496-4b62d4ea-b771-4aa2-a67f-1fdb02000947.png)

위 그림을 통해 방법론의 적용 결과를 예상해보면 아래와 같습니다.
- 데이터는 전체적으로 2개의 군집(왼쪽의 작은 이상치 군집, 오른쪽의 큰 정상데이터 군집)을 이루고 있는 것으로 볼 수 있으므로, gaussian density estimation과 MoG를 비교한다면 gaussian density estimation 보다는 MoG, 그 중에서도 2개의 정규분포를 사용하는 것이 더 좋은 성능을 달성할 것으로 생각된다.
- 두 개의 군집만이 존재한다고 가정하였을 때 정상 데이터 군집에도 이상치가 있는 경우가 있다는 점, 그리고 데이터의 모양을 보았을때 대체로 군집의 가장자리에 이상치가 포착된다는 점을 고려했을 때, LOF의 성능이 가장 좋을 것으로 생각된다.

위 예상이 맞는지를 확인해보면서 나머지 실험을 진행해봅시다.

### Gaussian density estimation VS Mixture of Gaussian
실험코드와 결과는 아래와 같습니다.
1. 전처리 및 실험준비: train set에 정상데이터만 넣고, test set에는 나머지 데이터를 모두 포함하는 식으로 전처리를 진행하였습니다.
```python
random.seed(221117)
trn_idx_set = list(random.sample(range(normal_data.shape[0]), round(normal_data.shape[0]*0.8)))
tst_idx_set = list(set(normal_data.index)-set(trn_idx_set))
trn_data = normal_data.iloc[trn_idx_set,:784].reset_index(drop=True)
tst_data = normal_data.iloc[tst_idx_set,:784]
tst_data = pd.concat([abnormal_data.iloc[:,:784], tst_data]).reset_index(drop=True)
tst_y = list(np.repeat(1,20))+list(np.repeat(0,len(tst_idx_set)))

eval_data = pd.DataFrame([[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]], 
columns=['n_gaussian', 'cov_type', 'time', 'tst_accuracy', 'tst_f1', 'tp', 'fn', 'tn', 'fp'])
```

2. 실험코드: gassian 분포의 수를 1(이 경우에는 그냥 Gaussian density estimation)에서 5까지 증가시켜보고, covariance matrix type을 앞에서 살펴봤던 full, diagonal, spherical로 바꾸면서 성능 및 training 시간의 변화를 확인하였습니다.
```python
from sklearn.mixture import GaussianMixture
import time
iter = 1
for n_gaussian in [1,2,3,4,5]:
    for cov_type in ['spherical', 'diag', 'full']:
        print('n_gaussian:{}, cov_type: {}'.format(n_gaussian, cov_type))
        st = time.time()
        model = GaussianMixture(n_components=n_gaussian, random_state=1117, covariance_type=cov_type).fit(trn_data)
        et = time.time()
        print('time:', et-st)
        y_pred = model.predict(tst_data)
        cm = confusion_matrix(tst_y, y_pred)
        tn = cm[0,0]
        fp = np.sum(cm[0,1:])
        fn = np.sum(cm[1:,0])
        tp = np.sum(cm[1:,1:])
        
        
        eval_data.iloc[iter-1, :] = [n_gaussian, cov_type, et-st, (tp+tn)/(tn+fp+fn+tp), 2*tp/(fp+2*tp+fn), tp, fn, tn, fp]

        iter = iter+1
```
실험결과는 아래와 같습니다.

![image](https://user-images.githubusercontent.com/112034941/202389619-99d03b98-0099-4e6f-a92e-44bd0728199d.png)
1. 시간: gaussian 분포의 수가 증가할수록, covariance type이 spherical -> full 일수록 걸리는 시간이 증가합니다. 
2. 성능: 성능은 정규분포의 수가 증가하여 2가 되었을때 최고의 성능을보이고 그 이후로는 일정한 성능을 보이며, covariance type이 diagonal일때의 성능이 일반적으로 높은 경우가 많습니다. 때문에 이를 통해 전체적으로 밀도가 높은 영역이 다수 존재할 것이라는 추론이 가능하되, 이 영역들은 정규분포로 근사하기 어려울 것으로 생각됩니다.
3. TP VS FP의 trade off 가 존재하는 것이 확연하고, 이는 아무래도 정규분포가정이라는 한계에 의한 것으로 해석됩니다. 정규분포에 이상치를 fitting하려다보면 대부분의 데이터를 포함하게 되는 상황이라고 볼 수 있기 때문입니다.

결론적으로, 정규분포 가정하에서의 밀도기반 이상치 탐지 방법론의 최고성능은 f1: 0.09이며, 앞서 TSNE를 통해 예상한 바와 일치함을 알 수 있습니다.

### Parzen window density estimation
실험코드와 결과는 아래와 같습니다.
1. 전처리 및 실험준비: train set에 정상데이터만 넣고, test set에는 나머지 데이터를 모두 포함하는 식으로 전처리를 진행하였습니다.
```python
random.seed(221117)
trn_idx_set = list(random.sample(range(normal_data.shape[0]), round(normal_data.shape[0]*0.8)))
tst_idx_set = list(set(normal_data.index)-set(trn_idx_set))
trn_data = normal_data.iloc[trn_idx_set,:784].reset_index(drop=True)
tst_data = normal_data.iloc[tst_idx_set,:784]
tst_data = pd.concat([abnormal_data.iloc[:,:784], tst_data]).reset_index(drop=True)
tst_y = list(np.repeat(1,20))+list(np.repeat(0,len(tst_idx_set)))
```

2. 실험코드: h는 0.5, 1, 10, 50/ kernel type은 'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear'/ cutoff는 하위 50위, 100위, 200위, 300위, 400위의 값으로 바꾸며 진행하였습니다.
```python
from sklearn.neighbors import KernelDensity
import copy
eval_data=[]
for h in [0.5,1,10,50]:
    for kernel_type in ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear']:
        print('h:{}, kernel_type: {}'.format(h, kernel_type))
        st = time.time()
        model = KernelDensity(kernel=kernel_type, bandwidth=h).fit(trn_data)
        y_prob = model.score_samples(tst_data)
        et = time.time()
        print('time:', et-st)
        tmp = copy.deepcopy(y_prob)
        tmp.sort()
        tmp_1 = copy.deepcopy(y_prob)
        for ranking in [50, 100, 200, 300, 400]:
            cutoff = tmp[ranking]
            y_pred = np.where(tmp_1<cutoff, 1, 0)
            cm = confusion_matrix(tst_y, y_pred)
            tn = cm[0,0]
            fp = cm[0,1]
            fn = cm[1,0]
            tp = cm[1,1]
            
            eval_data.append([h, kernel_type, et-st, ranking, (tp+tn)/(tn+fp+fn+tp), 2*tp/(fp+2*tp+fn), tp, fn, tn, fp])
eval_data = pd.DataFrame(eval_data, columns=['h', 'kernel_type', 'time', 'cutoff', 'tst_accuracy', 'tst_f1', 'tp', 'fn', 'tn', 'fp'])
eval_data.to_csv('PWDE_evaldf.csv')
```
실험결과는 아래와 같습니다.
1. h = 0.5 인 경우
![image](https://user-images.githubusercontent.com/112034941/202395450-8a6e84b4-e493-4fca-b92c-45f32526321b.png)

2. h = 1 인 경우
![image](https://user-images.githubusercontent.com/112034941/202396385-6713220a-4c8c-420b-a0a0-6c53b53c16c7.png)

3. h = 10 인 경우
![image](https://user-images.githubusercontent.com/112034941/202396514-20bf9c8f-a0bd-4a56-8d09-bf84e6c1ca2c.png)

4. h = 50 인 경우
![image](https://user-images.githubusercontent.com/112034941/202396703-04642134-04b0-41a3-ac8f-f02aa22d8fd6.png)

종합적으로 보았을때 눈에 띄는 점은 크게 두가지로 정리할 수 있겠습니다.
1. gaussian, exponential kernel을 제외한 kernel 경우에는 단 하나의 이상치도 탐지해내지 못했습니다. 이를 조금 더 자세히 분석해보기 위해 sklearn에서 제공한 각 커널의 변수 관점에서의 그래프 와 각 kernel에 따른 확률분포 근사 그래프를 보면 아래와 같습니다.
![image](https://user-images.githubusercontent.com/112034941/202397634-754cb2f9-3435-4d3a-a43b-c6a58779486d.png)
우선 kernel의 graph를 보면 이상치를 탐지해내지 못한 tophat, epanechinikov, linear kernel은 설정한 h를 넘어가면 0의 값을 부여하는 반면 이상치를 탐지해낸 커널의 경우 거리에 따라 더 적은 점수라도 부여하는 것을 알 수 있습니다. 앞서 t-SNE 결과를 통해 추측한 바에 따르면, 이상치는 군집의 가장자리에 있을 것으로 판단하였으므로 tophat 등과 같은 단절적인 kernel을 사용하면 이상치 탐지가 어려울 것이라고 나아가 추측가능한데, 그 추측이 현재 실험 결과와 들어맞는 것입니다. 

2. 일반적으로 성능은 cutoff가 증가하면 감소하는데, 이는 FP의 증가에 의한 것이며 최고성능은 exponential kernel, h=50, cutoff=50으로 하였을때 달성하였으나 전반적인 성능은 gaussian kernel에서 좋습니다. 이는 아무래도 h가 증가함에 따라 가우시안 커널은 완만해져서 이상치와 정상의 구분이 어려워지지만 exponential의 경우 정상쪽이 더 급격하게 높아지는 측면이 있어 h가 증가해도 가우시안 커널에 비해 덜 완만해지고 이에 따라 정상과 이상치의 구분이 조금 더 쉬워지는 것에 의한 결과로 해석됩니다. 특히 이는 앞서 추론한 바와 같이 현재 데이터의 이상치가 밀도가 높은 영역의 가장자리 지역에 있음을 가정하면 더 그럴 듯한 결과로 보이며, 해당 가정이 옳다는 가설에 더 힘을 실어주는 실험결과로 생각됩니다.

### Local Outlier Factor
실험코드와 결과는 아래와 같습니다.
1. 전처리 및 실험준비: train set에 정상데이터만 넣고, test set에는 나머지 데이터를 모두 포함하는 식으로 전처리를 진행하였습니다.
```python
random.seed(221117)
trn_idx_set = list(random.sample(range(normal_data.shape[0]), round(normal_data.shape[0]*0.8)))
tst_idx_set = list(set(normal_data.index)-set(trn_idx_set))
trn_data = normal_data.iloc[trn_idx_set,:784].reset_index(drop=True)
tst_data = normal_data.iloc[tst_idx_set,:784]
tst_data = pd.concat([abnormal_data.iloc[:,:784], tst_data]).reset_index(drop=True)
tst_y = list(np.repeat(1,20))+list(np.repeat(0,len(tst_idx_set)))
```

2. 실험코드: k 이웃의 k값을 2, 10, 50, 100, 200, 300, 400, 500으로 바꾸며 실험을 진행하였습니다.
```python
from sklearn.neighbors import LocalOutlierFactor
eval_data=[]
for k in [2, 10, 50, 100, 200, 300, 400, 500]:
    print(k)
    st = time.time()
    model = LocalOutlierFactor(n_neighbors=k, contamination=0.003, novelty=True).fit(trn_data)
    et = time.time()
    y_pred = model.predict(tst_data)
    y_pred = np.where(y_pred<0, 1, 0)
    cm = confusion_matrix(tst_y, y_pred)
    tn = cm[0,0]
    fp = cm[0,1]
    fn = cm[1,0]
    tp = cm[1,1]
    eval_data.append([k, et-st, (tp+tn)/(tn+fp+fn+tp), 2*tp/(fp+2*tp+fn), tp, fn, tn, fp])
eval_data = pd.DataFrame(eval_data, columns=['k', 'time', 'tst_accuracy', 'tst_f1', 'tp', 'fn', 'tn', 'fp'])
eval_data.to_csv('LOF_evaldf.csv')
```
실험결과는 아래와 같습니다.
![image](https://user-images.githubusercontent.com/112034941/202402345-1723ba30-71e5-4d68-af0b-562a1ed35bc4.png)

종합적으로 보았을때 눈에 띄는 점은 크게 두가지로 정리할 수 있겠습니다.
1. k가 증가함에 따라 성능도 증가하는 경향을 보이고, 걸리는 시간도 증가합니다. 이는 다소 당연한 결과로 보이는데, 앞서 살펴본 데이터의 가정에 따르면 고밀도 지역의 가장자리에 존재하는 데이터를 이상치로 분류하기 위해서는 어느 정도 높은 수준의 k-distance neiborhood를 정하는 것이 필요하기 때문입니다. 만약 k가 너무 작다면 고밀도 지역의 가장자리라는 특성이 잘 반영되지 않겠죠. 그냥 고밀도라고만 생각될테니 말입니다. 물론 이 논리에 따르면 k가 너무 커져도 안될텐데, 실제로 k가 실험해본 값들 중 중간 즈음의 값인 100일때 최고 성능 0.84를 달성하였습니다.
2. 성능이 앞서 보았던 parzen window 방법에 비해 높은 것(0.57 VS 0.84)이 눈에 띄는데, 이는 FP의 수가 매우 적어서 (30 vs 한자리 수) 그런 것으로 보입니다. 더불어 시간의 경우, LOF가 훨씬 짧은 것을 볼 수 있습니다. 만약 완벽하게 이상치를 찾아내고 싶은 것에 우선 순위가 있는 경우에는 parzen window 방법을, 빠르게 적당한 수의 이상치를 찾아내는 것이 목표라면 LOF의 사용이 적절할 것으로 보입니다. 

결론적으로, 밀도기반 이상치 탐지 방법론 중 현재 실험해본 데이터 셋에서는 LOF가 가장 좋은 성능을 나타냄을 알 수 있었으며, t-SNE를 통해 살펴보았던 이상치와 정상데이터에 대한 직관적 이해를 기반으로 한 추론에 부합하는 실험결과가 많이 나타난 것을 알 수 있었습니다.

이상으로 실험을 마무리하겠습니다.

---

## Conclusion
지금까지 밀도기반 anomaly detection의 이론적 배경, 개념, 그리고 이 이론적 내용을 증명하기 위한 실험 진행과정과 결과들을 살펴보았습니다. 그리고 t-SNE의 정사영 결과를 통해 추론한 바를 실제 실험 결과를 통해 증명해나가며 꽤 즐거운 실험을 해볼 수 있었는데, 이 tutorial을 보시는 분들에게도 그랬으면 좋겠습니다. 실제 실험의 진행 코드는 AD_tutorial.ipynb에 있으며, 제가 참고한 자료와 실습을 진행한 데이터셋은 올려두었습니다. 감사합니다.


